---
title: "Introductory exercise to ASReview LAB"
author: "The ASReview Academy Team"
---

## Introduction to the software ASReview LAB

The **goal** of this exercise is to get familiar with AI-aided screening
by making use of ASReview LAB v2.x.

You will learn how to install and set up the software, upload data,
screen records, and export and interpret the results. The exercise will
guide you through all the steps of AI-aided screening as described in
the [workflow on
Read-the-Docs](https://asreview.readthedocs.io/en/latest/lab/about.html#general-workflow-with-asreview){target="_blank"}.

Enjoy!

## Getting familiar

Before you start, you might want to read a bit more on:

-   [What is ASReview
    LAB](https://asreview.readthedocs.io/en/latest/lab/index.html){target="_blank"}?

-   [The terminology
    used](https://asreview.readthedocs.io/en/latest/lab/about.html#asreview-lab-terminology){target="_blank"}

-   The paper that was published in [Nature Machine
    Intelligence](https://www.nature.com/articles/s42256-020-00287-7){target="_blank"}

## The Software

### Step 1: Installing Python and ASReview LAB

First, you need to install [Python](https://www.python.org/downloads/){target="_blank"}.
The minimum Python version required is `Python 3.10`.

Once you have Python installed, you can go through the easy 3-step
[guide to installing (or upgrading)
ASReview](https://asreview.nl/local-installation/){target="_blank"} on the ASReview-website.

More detailed installation information, troubleshooting, and instructions to install
ASReview LAB on a server or via Docker are available on
[ReadTheDocs](https://asreview.readthedocs.io/en/latest/lab/installation.html){target="_blank"}.  

*Did you install the latest version of ASReview? You can proceed to
step 2!*

### Step 2: Starting ASReview LAB

To open ASReview LAB in your browser, you need to start it in the terminal. You can 
open your terminal by typing `terminal` (`cmd` on Windows 10 or lower) in your 
computer’s search bar (select `Run as administrator` if you have this option).

![](images/ASReviewLAB/step_1.png){width=85% fig-align="center"}

The terminal will open, in which you can type the following
command and press enter:

``` bash
asreview lab
```

Note that you have to keep your command-line interpreter running while
using ASReview LAB, even though the interface is in your browser!

![](images/ASReviewLAB/step_2.png){width=85% fig-align="center"}

It takes a few seconds for ELAS - your Electronic Learning Assistant -
to start the software. It will appear in your (default) web browser. 

But why do you need to start it up by running code in your command
prompt? This ensures that ASReview LAB runs locally. More
specifically, <u>your data is and stays your own</u>. Small price to pay
for complete privacy, right?! Read more about the key principles in the
[Zen of Elas](https://asreview.nl/blog/the-zen-of-elas/){target="_blank"}!

You can also run the software via a server, but you need to take care of
[hosting the server
yourself](https://asreview.readthedocs.io/en/latest/lab/installation.html#server-installation){target="_blank"}
(or ask your IT-department).


*Did ASReview LAB open in your browser? If so, you can proceed to
step 3!*

## Project Setup

### Step 3: Selecting a Dataset

Without data, we have nothing to screen. So, you need to tell ELAS
which dataset you want to screen for relevant articles.

For this exercise we are screening in the so-called `Validation Mode`
of ASReview. By screening in the [Validation
Mode](https://asreview.readthedocs.io/en/latest/lab/validation.html){target="_blank"},
we are going to make use of a [benchmark
dataset](https://asreview.readthedocs.io/en/latest/lab/project_create.html#from-discovery){target="_blank"}.
This means that all records in the dataset have already been labeled as
relevant or irrelevant.

Click on the `Discover` button, highlighted with a green circle.

![](images/ASReviewLAB/step_3a.png){width=85% fig-align="center"}

This will show you a list of 26 benchmark datasets that are already labeled. In this example we will use the 
`van de Schoot et al. (2018)` dataset about PTSD trajectories. Scroll down to find the dataset, and click on it.

![](images/ASReviewLAB/step_3b.png){width=85% fig-align="center"}

Then, you will be shown a menu with a donwload button. Click `Download`, and wait until the download finishes. When
the download finishes, a new menu pops up which brings us to the next step: Setting up the project.

![](images/ASReviewLAB/step_3c.png){width=85% fig-align="center"}

*Did you successfully download a dataset? If so, you can proceed to step 4!*

### Step 4: Setting up a Project

Now that the dataset is downloaded, we can continue with setting up the project. First, click on the pencil
icon at the top of this menu to change the name of your project. You can also leave it like it is. Then
you can click on `Screen` which creates a project for you. 

![](images/ASReviewLAB/step_4.png){width=85% fig-align="center"}

More detailed information about setting up a project can be found on
[ReadTheDocs](https://asreview.readthedocs.io/en/latest/lab/project_create.html){target="_blank"}.
 
*Did you successfully create a project, and can you see the first record? If so, you can proceed to step 5!*

### Step 5: Prior knowledge

Before you can start screening the records, you need to tell ELAS what
kind of records you <u>are</u> and what kind of records you <u>are
not</u> looking for (i.e., relevant and irrelevant records,
respectively). We call this *prior knowledge*. Based on the prior
knowledge you provide, ELAS will reorder the stack of papers and provide
you with the record that is most likely to be relevant (default
settings).

When performing a systematic review with your own data, you need to
provide the prior knowledge yourself (at least one relevant and one
irrelevant record). However, because you are using the Validation Mode 
of ASReview, the relevant records are known; the original authors have 
already read ALL records. See also the documentation 
about the selection of [prior knowledge](https://asreview.readthedocs.io/en/latest/lab/project_create.html#prior-knowledge){target="_blank"}.

To select the prior knowledge you first need to click on the `Customize`
button on the left side of the screen. The customization window will open.

![](images/ASReviewLAB/step_5a.png){width=85% fig-align="center"}

On the customization window, scroll down to `Prior knowledge`, and click on the `Search` button.

![](images/ASReviewLAB/step_5b.png){width=85% fig-align="center"}

Now you will see a pop up that you can use to select prior knowledge.

The following five papers are known to be relevant:

-   Latent Trajectories of Trauma Symptoms and Resilience 
    (DOI: [10.4088/JCP.13m08914](https://doi.org/10.4088/jcp.13m08914){target="_blank"})
-   A Latent Growth Mixture Modeling Approach to PTSD Symptoms in Rape
    Victims (DOI: [10.1177/1534765610395627](https://doi.org/10.1177/1534765610395627){target="_blank"})
-   Peace and War: Trajectories of Posttraumatic Stress Disorder
    Symptoms Before, During, and After Military Deployment in
    Afghanistan (DOI: [10.1177/0956797612457389](https://doi.org/10.1177/0956797612457389){target="_blank"})
-   The relationship between course of PTSD symptoms in deployed U.S.
    Marines and degree of combat exposure (DOI: [10.1002/jts.21988](https://doi.org/10.1002/jts.21988){target="_blank"})
-   Trajectories of trauma symptoms and resilience in deployed US
    military service members: Prospective cohort study (DOI: [10.1192/bjp.bp.111.096552](https://doi.org/10.1192/bjp.bp.111.096552){target="_blank"})

To add the relevant records, you copy and paste the titles
of these relevant records one by one in the search bar and add them as
relevant. You should add between 1 and 5 relevant priors from this list.
When you are done, go back to the Reviewer screen by clicking on the `Reviewer` button
on the left.

![](images/ASReviewLAB/step_5c.png){width=85% fig-align="center"}

*Did you select between 1 and 5 relevant records? If so, you
can proceed to step 6!*

## Screening phase

### Step 6: Screening the Records

Everything is set up and ready to screen, well done!

Since we are in the Validation Mode of ASReview, you can pretend to be an
expert on the topic of PTSD and pretend you have all the knowledge of the
original screeners. All records in the dataset have been labeled as
relevant/irrelevant, which is indicated through a banner below each article.
Click on the Relevant button (or press `R` on your keyboard) if the record 
is marked as relevant. If not, you can press the Irrelevant button (or `I` 
on your keyboard).

![](images/ASReviewLAB/step_6.png){width=85% fig-align="center"}

Now, all we need is a Stopping Rule to determine when you are confident
that you have identified (almost) all relevant records in your dataset.
For this exercise, continue screening records until you have marked *50
consecutive records as irrelevant*. You can check up on your progress in
the [Dashboard
page](https://asreview.readthedocs.io/en/latest/lab/progress.html){target="_blank"}.

When you have reached your Stopping Rule and you are done screening, go
back to the Dashboard. Here you can see the summary statistics of
your project such as the number of records you have labeled relevant or
irrelevant. It also shows how many records are in your dataset and how
many records you labeled irrelevant since you have screened the last
relevant record. For more information about how to read these summary
statistics and interpret the corresponding charts, check out the
[documentation](https://asreview.readthedocs.io/en/latest/lab/progress.html){target="_blank"}.

The Van de Schoot (2018) dataset contains 38 relevant records in this
particular example. Did you get to label all of them as relevant before
you reached your Stopping Rule? If you did, great!

What is the percentage of total papers you needed to screen to find the
number of relevant records you have found? Is it \<100%? Then, you were
quicker compared to the original screeners of the dataset!

You probably had to screen about only 2-3% of the data. Amazing right?!
Chances are though that you did not get to see all of the relevant
records before you stopped screening. Do you think this is acceptable?
There is a trade-off between the time spent screening and the error
rate: the more records you screen, the lower the risk of missing a
relevant record. However, screening all records in your dataset is still
no guarantee for an error rate of zero, since even traditional screening
by humans - which is the gold standard to which we compare AI-assisted
screening - is not perfect [^1].

Your willingness to accept the risk that you may exclude some relevant
records is something to take into account when deciding on a Stopping
Rule. Read more about Stopping Rules and how to decide on a good
strategy for your data on the [discussion
platform](https://github.com/asreview/asreview/discussions){target="_blank"}. 

### Step 7: Extracting and inspecting the data

Now that you found all or most relevant records, you can export your
data using [these
instructions](https://asreview.readthedocs.io/en/latest/lab/progress.html#export-dataset){target="_blank"}.
If you choose to inspect your data in Excel, download the data in
`Excel` format. If you prefer to inspect your data in R or Python, download the
`CSV (UTF-8)` format and open it in R or Python.

You can find all the data that was originally imported to ASReview in
the exported data file, in a new order and with two new columns added at
the end.

Using the information about the [Read the Docs
page](https://asreview.readthedocs.io/en/latest/lab/progress.html#export-dataset){target="_blank"}
can you reorder your data to appear in the order in which you loaded
them into ASReview? And back to the order provided by ASReview?

Check if the number of records coded `asreview_label = 1` corresponds to the number
of relevant records on the Dashboard. From which row number, based on
the original ordering, do the included articles come from?

For the last exercise, it is important to change the order back to the order
provided by ASReview. Lastly, check out the first few records with no number
in the `included` column. Are those articles labeled as `relevant` in the
original dataset? (Whether or not a record was pre-labeled as relevant is
shown in the column `label_included` in the original dataset.)

## Goal

In the beginning of the LAB the following goal was specified: “The
**goal** of this LAB is to get familiar with AI-aided screening by
making use of ASReview LAB.”
Did you achieve this goal?

If so: **congratulations!** You now know all the steps to create and
screen for a systematic review. ELAS wishes you a lot of fun screening
with ASReview!  
Do you like the software, leave a star on
[Github](https://github.com/asreview/asreview){target="_blank"}; this will help to
increase the visibility of the open-source project.

## What’s next?

Some suggestions:

-   Read a blog posts about:

    -   [Five ways to get involved in
        ASReview](https://asreview.nl/blog/open-source-and-research/){target="_blank"},
    -   [Seven ways to integrate ASReview in your systematic review
        workflow](https://asreview.nl/blog/seven-ways-to-integrate-asreview/){target="_blank"},
    -   [Active learning
        explained](https://asreview.nl/blog/active-learning-explained/){target="_blank"}.

-   Ready to start your own project? Upload [your own
    data](https://asreview.readthedocs.io/en/latest/lab/data.html){target="_blank"} and start
    screening in the Oracle mode!

-   Interested in learning what other AI models ASReview offers? Read the 
    [documentation](https://asreview.readthedocs.io/en/latest/lab/models.html){target="_blank"} 
    and try to switch models in the `Customize` screen!

-   Try to find the hidden memory game in ASReview (some people found it
    by going through the [source
    code](https://github.com/asreview/asreview/tree/master/asreview){target="_blank"} on
    Github… +1 for open-science!)


[^1]: Wang Z, Nayfeh T, Tetzlaff J, O’Blenis P, Murad MH (2020) Error
    rates of human reviewers during abstract screening in systematic
    reviews. PLOS ONE 15(1): e0227742.
    <[https://doi.org/10.1371/journal.pone.0227742](https://doi.org/10.1371/journal.pone.0227742){target="_blank"}>